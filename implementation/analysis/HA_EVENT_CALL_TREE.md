# Home Assistant Event Call Tree Analysis
## Complete Data Flow: HA â†’ Database â†’ Dashboard

**Document Version**: 1.0  
**Created**: 2025-10-13  
**Purpose**: Detailed call tree showing complete event flow from Home Assistant through the entire system

---

## ğŸ“Š Overview

This document traces the complete journey of a Home Assistant event from its origin through processing, storage, and display on the dashboard. The flow involves multiple services working together in a microservices architecture.

### Architecture Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Home Assistant  â”‚ (External System)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ WebSocket Connection (WSS/WS)
         â”‚ Event: state_changed, call_service, etc.
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebSocket Ingestion Service (Port 8001)       â”‚
â”‚ - Connection Management                        â”‚
â”‚ - Event Subscription                           â”‚
â”‚ - Initial Processing                           â”‚
â”‚ - Async Queue Management                       â”‚
â”‚ - Batch Processing                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º (Optional) Enrichment Pipeline (Port 8002)
         â”‚            - Data Normalization
         â”‚            - Data Validation
         â”‚            - Quality Metrics
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InfluxDB (Port 8086)                          â”‚
â”‚ - Time-Series Database                         â”‚
â”‚ - Measurements: home_assistant_events          â”‚
â”‚ - Retention: 1 year raw, 5 years aggregated   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Flux Query Language
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Admin API Service (Port 8003)                 â”‚
â”‚ - REST API Gateway                             â”‚
â”‚ - Statistics Endpoints                         â”‚
â”‚ - Events Endpoints                             â”‚
â”‚ - WebSocket Streaming                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTP/REST + WebSocket
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Health Dashboard (Port 3000)                  â”‚
â”‚ - React Frontend                               â”‚
â”‚ - Real-time Updates                            â”‚
â”‚ - 12 Tabs with Visualizations                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Detailed Call Tree

### Phase 1: Event Reception from Home Assistant

#### 1.1 WebSocket Connection Establishment

**File**: `services/websocket-ingestion/src/connection_manager.py`

```python
ConnectionManager.connect()
â””â”€â–º ConnectionManager._connect_with_retry()
    â””â”€â–º HomeAssistantWebSocketClient.connect()
        â”œâ”€â–º websocket.connect(url)  # aiohttp WebSocket
        â”œâ”€â–º _authenticate()
        â”‚   â”œâ”€â–º send_message({"type": "auth", "access_token": token})
        â”‚   â””â”€â–º receive auth_ok/auth_invalid
        â””â”€â–º _on_connect()
            â”œâ”€â–º _subscribe_to_events()
            â”‚   â””â”€â–º EventSubscriptionManager.subscribe_to_event("state_changed")
            â”‚       â”œâ”€â–º generate subscription ID (message_id counter)
            â”‚       â”œâ”€â–º send_message({
            â”‚       â”‚     "id": message_id,
            â”‚       â”‚     "type": "subscribe_events",
            â”‚       â”‚     "event_type": "state_changed"
            â”‚       â”‚   })
            â”‚       â””â”€â–º wait for subscription confirmation
            â”‚
            â””â”€â–º DiscoveryService.discover_all()
                â”œâ”€â–º discover_devices()
                â””â”€â–º discover_entities()
```

**Key Data Structures**:
- **Connection URL**: `ws://HA_URL:8123/api/websocket`
- **Auth Token**: Long-lived access token from Home Assistant
- **Subscription Types**: `state_changed`, `call_service`, etc.

---

#### 1.2 Event Message Reception

**File**: `services/websocket-ingestion/src/websocket_client.py`

```python
HomeAssistantWebSocketClient.listen()
â””â”€â–º async for msg in self.websocket:  # aiohttp WebSocket message loop
    â”œâ”€â–º if msg.type == WSMsgType.TEXT:
    â”‚   â”œâ”€â–º json.loads(msg.data)  # Parse JSON message
    â”‚   â””â”€â–º if self.on_message:
    â”‚       â””â”€â–º on_message(data)  # Callback to ConnectionManager
    â”‚
    â””â”€â–º ConnectionManager._handle_message(message)
        â”œâ”€â–º if message["type"] == "event":
        â”‚   â””â”€â–º EventSubscriptionManager.handle_event_message(message)
        â”‚       â”œâ”€â–º Extract event data from message["event"]
        â”‚       â”œâ”€â–º Log event reception
        â”‚       â””â”€â–º Trigger event handlers
        â”‚           â””â”€â–º for handler in self.event_handlers:
        â”‚               â””â”€â–º handler(event_data)  # Call to main service
        â”‚
        â””â”€â–º if message["type"] == "result":
            â””â”€â–º EventSubscriptionManager.handle_subscription_result(message)
```

**Event Message Structure** (from Home Assistant):
```json
{
  "id": 123,
  "type": "event",
  "event": {
    "event_type": "state_changed",
    "data": {
      "entity_id": "sensor.temperature",
      "old_state": {
        "state": "20.5",
        "attributes": {"unit_of_measurement": "Â°C"}
      },
      "new_state": {
        "state": "21.0",
        "attributes": {"unit_of_measurement": "Â°C"}
      }
    },
    "origin": "LOCAL",
    "time_fired": "2025-10-13T10:30:00.123456+00:00",
    "context": {
      "id": "01234567890abcdef",
      "user_id": null
    }
  }
}
```

---

### Phase 2: Event Processing & Queue Management

#### 2.1 Event Handler Callback

**File**: `services/websocket-ingestion/src/main.py`

```python
WebSocketIngestionService.start()
â”œâ”€â–º connection_manager.on_message = self._handle_event
â”‚
â””â”€â–º _handle_event(message)  # Called by WebSocket client
    â”œâ”€â–º if message["type"] == "event":
    â”‚   â”œâ”€â–º event_data = message["event"]
    â”‚   â”œâ”€â–º EventProcessor.validate_event(event_data)
    â”‚   â”‚   â”œâ”€â–º Check required fields (entity_id, event_type, time_fired)
    â”‚   â”‚   â”œâ”€â–º Validate data structure
    â”‚   â”‚   â””â”€â–º Return bool (valid/invalid)
    â”‚   â”‚
    â”‚   â”œâ”€â–º if valid:
    â”‚   â”‚   â”œâ”€â–º EventProcessor.extract_event_data(event_data)
    â”‚   â”‚   â”‚   â”œâ”€â–º _extract_state_changed_data()
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â–º Parse entity_id â†’ extract domain
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â–º Extract old_state and new_state
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â–º Extract attributes (unit_of_measurement, device_class, etc.)
    â”‚   â”‚   â”‚   â”‚   â””â”€â–º Extract context (user_id, parent_id)
    â”‚   â”‚   â”‚   â”‚
    â”‚   â”‚   â”‚   â””â”€â–º Return structured event data
    â”‚   â”‚   â”‚
    â”‚   â”‚   â””â”€â–º AsyncEventProcessor.process_event(extracted_data)
    â”‚   â”‚       â””â”€â–º event_queue.put_nowait(event_data)  # Non-blocking queue
    â”‚   â”‚
    â”‚   â””â”€â–º log event reception statistics
    â”‚
    â””â”€â–º return processed status
```

**Extracted Event Data Structure**:
```python
{
    "event_type": "state_changed",
    "entity_id": "sensor.temperature",
    "domain": "sensor",
    "time_fired": "2025-10-13T10:30:00.123456+00:00",
    "origin": "LOCAL",
    "context": {
        "id": "01234567890abcdef",
        "user_id": null
    },
    "old_state": {
        "state": "20.5",
        "attributes": {"unit_of_measurement": "Â°C"},
        "last_changed": "2025-10-13T10:25:00.000000+00:00",
        "last_updated": "2025-10-13T10:25:00.000000+00:00"
    },
    "new_state": {
        "state": "21.0",
        "attributes": {"unit_of_measurement": "Â°C"},
        "last_changed": "2025-10-13T10:30:00.123456+00:00",
        "last_updated": "2025-10-13T10:30:00.123456+00:00"
    }
}
```

---

#### 2.2 Async Event Processing

**File**: `services/websocket-ingestion/src/async_event_processor.py`

```python
AsyncEventProcessor (Background Workers)
â”œâ”€â–º start()
â”‚   â””â”€â–º for i in range(max_workers):  # Default: 10 workers
â”‚       â””â”€â–º asyncio.create_task(_worker(f"worker-{i}"))
â”‚
â””â”€â–º _worker(worker_name)  # Runs continuously
    â””â”€â–º while self.is_running:
        â”œâ”€â–º event_data = await self.event_queue.get()  # Blocks until event available
        â”œâ”€â–º RateLimiter.acquire()  # Max 1000 events/second
        â”‚
        â”œâ”€â–º for handler in self.event_handlers:
        â”‚   â””â”€â–º await handler(event_data)
        â”‚       â””â”€â–º BatchProcessor.add_event(event_data)
        â”‚
        â”œâ”€â–º self.processed_events += 1
        â”œâ”€â–º self.processing_times.append(processing_time)
        â””â”€â–º self.event_queue.task_done()
```

**Performance Characteristics**:
- **Concurrency**: 10 parallel workers
- **Queue Size**: 10,000 events max
- **Rate Limit**: 1,000 events/second
- **Processing Time**: Tracked for last 1,000 events

---

#### 2.3 Batch Processing

**File**: `services/websocket-ingestion/src/batch_processor.py`

```python
BatchProcessor
â”œâ”€â–º add_event(event_data)
â”‚   â”œâ”€â–º async with self.batch_lock:
â”‚   â”‚   â”œâ”€â–º self.current_batch.append(event_data)
â”‚   â”‚   â”œâ”€â–º if len(current_batch) >= batch_size (1000):
â”‚   â”‚   â”‚   â””â”€â–º _process_batch()
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º if batch_timeout (5 seconds) exceeded:
â”‚   â”‚       â””â”€â–º _process_batch()
â”‚   â”‚
â”‚   â””â”€â–º return queued status
â”‚
â””â”€â–º _process_batch()
    â”œâ”€â–º batch_to_process = self.current_batch
    â”œâ”€â–º self.current_batch = []  # Clear for next batch
    â”‚
    â”œâ”€â–º for handler in self.batch_handlers:
    â”‚   â””â”€â–º await handler(batch_to_process)
    â”‚       â””â”€â–º InfluxDBBatchWriter.write_batch(events)
    â”‚
    â””â”€â–º log batch statistics
```

**Batch Configuration**:
- **Batch Size**: 1,000 events
- **Batch Timeout**: 5 seconds
- **Retry Logic**: 3 attempts with exponential backoff

---

### Phase 3: Database Write Operations

#### 3.1 InfluxDB Schema Creation

**File**: `services/websocket-ingestion/src/influxdb_schema.py`

```python
InfluxDBSchema.create_event_point(event_data)
â”œâ”€â–º Extract basic fields:
â”‚   â”œâ”€â–º event_type = event_data["event_type"]
â”‚   â”œâ”€â–º entity_id = event_data["entity_id"]
â”‚   â”œâ”€â–º timestamp = parse_timestamp(event_data["time_fired"])
â”‚   â””â”€â–º domain = entity_id.split(".")[0]
â”‚
â”œâ”€â–º Point(measurement="home_assistant_events")
â”‚   â”œâ”€â–º .time(timestamp, WritePrecision.MS)
â”‚   â”‚
â”‚   â”œâ”€â–º _add_event_tags(point, event_data)  # Indexed for fast queries
â”‚   â”‚   â”œâ”€â–º .tag("entity_id", entity_id)
â”‚   â”‚   â”œâ”€â–º .tag("domain", domain)
â”‚   â”‚   â”œâ”€â–º .tag("event_type", event_type)
â”‚   â”‚   â”œâ”€â–º .tag("device_class", attributes.get("device_class"))
â”‚   â”‚   â”œâ”€â–º .tag("area", attributes.get("area"))
â”‚   â”‚   â””â”€â–º .tag("location", attributes.get("location"))
â”‚   â”‚
â”‚   â””â”€â–º _add_event_fields(point, event_data)  # Actual data values
â”‚       â”œâ”€â–º .field("state", new_state["state"])
â”‚       â”œâ”€â–º .field("old_state", old_state["state"])
â”‚       â”œâ”€â–º .field("attributes", json.dumps(attributes))
â”‚       â”œâ”€â–º .field("context_id", context["id"])
â”‚       â”œâ”€â–º .field("context_user_id", context["user_id"])
â”‚       â”œâ”€â–º Extract numeric values if applicable:
â”‚       â”‚   â”œâ”€â–º .field("temperature", float(state)) if domain == "sensor"
â”‚       â”‚   â”œâ”€â–º .field("humidity", float(state)) if device_class == "humidity"
â”‚       â”‚   â””â”€â–º .field("pressure", float(state)) if device_class == "pressure"
â”‚       â”‚
â”‚       â””â”€â–º return Point object
â”‚
â””â”€â–º return point
```

**InfluxDB Point Structure**:
```
Measurement: home_assistant_events
Tags (indexed):
  - entity_id: "sensor.temperature"
  - domain: "sensor"
  - event_type: "state_changed"
  - device_class: "temperature"
  - area: "living_room"
Fields (data):
  - state: "21.0"
  - old_state: "20.5"
  - temperature: 21.0 (numeric)
  - attributes: '{"unit_of_measurement":"Â°C",...}'
  - context_id: "01234567890abcdef"
Timestamp: 2025-10-13T10:30:00.123Z
```

---

#### 3.2 InfluxDB Batch Write

**File**: `services/websocket-ingestion/src/influxdb_batch_writer.py`

```python
InfluxDBBatchWriter.write_batch(events)
â”œâ”€â–º for event in events:
â”‚   â”œâ”€â–º point = InfluxDBSchema.create_event_point(event)
â”‚   â””â”€â–º batch_points.append(point)
â”‚
â”œâ”€â–º async with self.batch_lock:
â”‚   â”œâ”€â–º InfluxDBConnectionManager.get_write_api()
â”‚   â”‚   â””â”€â–º influxdb_client.write_api(
â”‚   â”‚         write_options=WriteOptions(
â”‚   â”‚           batch_size=1000,
â”‚   â”‚           flush_interval=5000
â”‚   â”‚         )
â”‚   â”‚       )
â”‚   â”‚
â”‚   â”œâ”€â–º write_api.write(
â”‚   â”‚     bucket=self.bucket,
â”‚   â”‚     org=self.org,
â”‚   â”‚     record=batch_points,
â”‚   â”‚     write_precision=WritePrecision.MS
â”‚   â”‚   )
â”‚   â”‚
â”‚   â”œâ”€â–º await write_api.flush()  # Ensure data is written
â”‚   â”‚
â”‚   â””â”€â–º on_success:
â”‚       â”œâ”€â–º self.total_batches_written += 1
â”‚       â”œâ”€â–º self.total_points_written += len(batch_points)
â”‚       â””â”€â–º log batch statistics
â”‚
â””â”€â–º on_error:
    â”œâ”€â–º retry with exponential backoff (max 3 attempts)
    â”œâ”€â–º self.total_points_failed += len(batch_points)
    â””â”€â–º log error details
```

**Write Performance**:
- **Batch Size**: Up to 1,000 points
- **Flush Interval**: 5 seconds
- **Write Precision**: Milliseconds
- **Retry Strategy**: Exponential backoff (1s, 2s, 4s)

---

### Phase 4: Optional Enrichment Pipeline

#### 4.1 Enrichment Service Processing

**File**: `services/enrichment-pipeline/src/main.py`

```python
EnrichmentPipelineService.process_event(event_data)
â”œâ”€â–º DataNormalizer.normalize_event(event_data)
â”‚   â”œâ”€â–º DataValidator.validate_event(event_data)
â”‚   â”‚   â”œâ”€â–º Check required fields
â”‚   â”‚   â”œâ”€â–º Validate data types
â”‚   â”‚   â”œâ”€â–º Check value ranges
â”‚   â”‚   â””â”€â–º Return ValidationResult(is_valid, errors, warnings)
â”‚   â”‚
â”‚   â”œâ”€â–º normalized = event_data.copy()
â”‚   â”‚
â”‚   â”œâ”€â–º _normalize_timestamps(normalized)
â”‚   â”‚   â”œâ”€â–º Convert to ISO 8601 format
â”‚   â”‚   â”œâ”€â–º Ensure UTC timezone
â”‚   â”‚   â””â”€â–º Add "_normalized" metadata
â”‚   â”‚
â”‚   â”œâ”€â–º _normalize_state_values(normalized)
â”‚   â”‚   â”œâ”€â–º Boolean states: "on" â†’ True, "off" â†’ False
â”‚   â”‚   â”œâ”€â–º Numeric states: "21.5" â†’ 21.5 (float)
â”‚   â”‚   â””â”€â–º String states: trim whitespace
â”‚   â”‚
â”‚   â”œâ”€â–º _normalize_units(normalized)
â”‚   â”‚   â”œâ”€â–º Temperature: Â°C â†’ celsius, Â°F â†’ fahrenheit
â”‚   â”‚   â”œâ”€â–º Pressure: hPa â†’ hectopascal
â”‚   â”‚   â””â”€â–º Standardize unit names
â”‚   â”‚
â”‚   â””â”€â–º return normalized event
â”‚
â””â”€â–º InfluxDBClientWrapper.write_normalized_event(normalized)
    â””â”€â–º (Similar to Phase 3 write process)
```

**Normalization Benefits**:
- **Consistent Data Types**: String â†’ Numeric/Boolean where applicable
- **Standardized Units**: Unified unit naming conventions
- **Validation**: Early detection of data quality issues
- **Metadata**: Tracking of normalization version and timestamp

---

### Phase 5: Data Retrieval by Admin API

#### 5.1 API Request Handling

**File**: `services/admin-api/src/events_endpoints.py`

```python
EventsEndpoints (FastAPI Router)
â”œâ”€â–º @router.get("/events")
â”‚   â””â”€â–º async def get_recent_events(
â”‚         limit: int = 100,
â”‚         entity_id: Optional[str] = None,
â”‚         event_type: Optional[str] = None,
â”‚         start_time: Optional[datetime] = None,
â”‚         end_time: Optional[datetime] = None
â”‚       )
â”‚       â”œâ”€â–º Build EventFilter object
â”‚       â”œâ”€â–º _get_all_events(filter, limit, offset)
â”‚       â”‚   â””â”€â–º InfluxDBClientWrapper.query_events(filter)
â”‚       â”‚       â”œâ”€â–º Build Flux query
â”‚       â”‚       â”‚   ```flux
â”‚       â”‚       â”‚   from(bucket: "ha_events")
â”‚       â”‚       â”‚     |> range(start: -1h)
â”‚       â”‚       â”‚     |> filter(fn: (r) => r._measurement == "home_assistant_events")
â”‚       â”‚       â”‚     |> filter(fn: (r) => r.entity_id == "sensor.temperature")
â”‚       â”‚       â”‚     |> sort(columns: ["_time"], desc: true)
â”‚       â”‚       â”‚     |> limit(n: 100)
â”‚       â”‚       â”‚   ```
â”‚       â”‚       â”‚
â”‚       â”‚       â”œâ”€â–º query_api.query(query)
â”‚       â”‚       â”œâ”€â–º Parse FluxTable results
â”‚       â”‚       â”‚   â”œâ”€â–º Extract tags (entity_id, domain, event_type)
â”‚       â”‚       â”‚   â”œâ”€â–º Extract fields (state, attributes)
â”‚       â”‚       â”‚   â”œâ”€â–º Extract timestamp
â”‚       â”‚       â”‚   â””â”€â–º Construct EventData objects
â”‚       â”‚       â”‚
â”‚       â”‚       â””â”€â–º return List[EventData]
â”‚       â”‚
â”‚       â””â”€â–º return JSON response
â”‚
â””â”€â–º @router.get("/events/stats")
    â””â”€â–º async def get_events_stats(period: str = "1h")
        â””â”€â–º InfluxDBClientWrapper.get_event_statistics(period)
            â”œâ”€â–º Query: Count events by domain
            â”œâ”€â–º Query: Count events by type
            â”œâ”€â–º Query: Calculate event rate
            â””â”€â–º return aggregated statistics
```

**API Response Structure**:
```json
[
  {
    "event_id": "01234567890abcdef",
    "event_type": "state_changed",
    "entity_id": "sensor.temperature",
    "domain": "sensor",
    "timestamp": "2025-10-13T10:30:00.123Z",
    "state": "21.0",
    "old_state": "20.5",
    "attributes": {
      "unit_of_measurement": "Â°C",
      "device_class": "temperature",
      "friendly_name": "Living Room Temperature"
    },
    "context": {
      "id": "01234567890abcdef",
      "user_id": null
    }
  }
]
```

---

#### 5.2 Statistics Aggregation

**File**: `services/admin-api/src/stats_endpoints.py`

```python
StatsEndpoints
â”œâ”€â–º @router.get("/stats")
â”‚   â””â”€â–º async def get_statistics(period: str = "1h")
â”‚       â””â”€â–º _get_stats_from_influxdb(period)
â”‚           â”œâ”€â–º get_event_statistics(period)
â”‚           â”‚   â”œâ”€â–º Flux query: Count total events
â”‚           â”‚   â”œâ”€â–º Flux query: Count by domain
â”‚           â”‚   â”œâ”€â–º Flux query: Count by event_type
â”‚           â”‚   â””â”€â–º Flux query: Calculate percentiles (p50, p95, p99)
â”‚           â”‚
â”‚           â”œâ”€â–º get_error_rate(period)
â”‚           â”‚   â””â”€â–º Flux query: Error events / Total events
â”‚           â”‚
â”‚           â”œâ”€â–º get_service_metrics(service, period)
â”‚           â”‚   â”œâ”€â–º Flux query: Event processing time
â”‚           â”‚   â”œâ”€â–º Flux query: Queue depth
â”‚           â”‚   â””â”€â–º Flux query: Throughput rate
â”‚           â”‚
â”‚           â””â”€â–º Combine all metrics into response
â”‚               â””â”€â–º return {
â”‚                     "total_events": 12500,
â”‚                     "event_rate": 3.47,  # events/second
â”‚                     "domains": {...},
â”‚                     "error_rate": 0.02,  # 2%
â”‚                     "metrics": {...}
â”‚                   }
â”‚
â””â”€â–º @router.get("/stats/trends")
    â””â”€â–º async def get_trends(period: str = "24h")
        â””â”€â–º Query time-series data with window aggregation
            â”œâ”€â–º Flux query: Window by 5-minute intervals
            â””â”€â–º Return trend data for charting
```

**Flux Query Example** (Event Count by Domain):
```flux
from(bucket: "ha_events")
  |> range(start: -1h)
  |> filter(fn: (r) => r._measurement == "home_assistant_events")
  |> group(columns: ["domain"])
  |> count()
  |> group()
```

---

### Phase 6: Frontend Data Display

#### 6.1 API Service Layer

**File**: `services/health-dashboard/src/services/api.ts`

```typescript
class ApiService {
  private baseURL = 'http://localhost:8003/api';
  
  async getStatistics(period: string = '1h'): Promise<Statistics> {
    â””â”€â–º fetch(`${baseURL}/stats?period=${period}`)
        â”œâ”€â–º Add headers: { 'Content-Type': 'application/json' }
        â”œâ”€â–º await response.json()
        â”œâ”€â–º Validate response structure
        â””â”€â–º return typed Statistics object
  }
  
  async getServicesHealth(): Promise<{ [key: string]: any }> {
    â””â”€â–º fetch(`${baseURL}/health/services`)
        â””â”€â–º return health status for all services
  }
  
  async getRecentEvents(options: EventQueryOptions): Promise<Event[]> {
    â””â”€â–º Build query string with filters
        â””â”€â–º fetch(`${baseURL}/events?${queryString}`)
            â””â”€â–º return typed Event[] array
  }
}

export const apiService = new ApiService();
```

---

#### 6.2 React Hooks for Data Fetching

**File**: `services/health-dashboard/src/hooks/useStatistics.ts`

```typescript
export const useStatistics = (period: string, refreshInterval: number = 60000) => {
  const [statistics, setStatistics] = useState<Statistics | null>(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  
  const fetchStatistics = async () => {
    â””â”€â–º try:
        â”œâ”€â–º setError(null)
        â”œâ”€â–º const statsData = await apiService.getStatistics(period)
        â””â”€â–º setStatistics(statsData)
        catch:
        â””â”€â–º setError(error.message)
        finally:
        â””â”€â–º setLoading(false)
  };
  
  useEffect(() => {
    â””â”€â–º fetchStatistics()  // Initial fetch
        â””â”€â–º setInterval(fetchStatistics, refreshInterval)  // Polling
            â””â”€â–º return cleanup function
  }, [period, refreshInterval]);
  
  return { statistics, loading, error, refresh: fetchStatistics };
};
```

**Hook Usage Pattern**:
```typescript
// In React Component
const { statistics, loading, error } = useStatistics('1h', 60000);

// statistics updates every 60 seconds automatically
// Component re-renders with new data
```

---

#### 6.3 Dashboard Component Rendering

**File**: `services/health-dashboard/src/components/Dashboard.tsx`

```typescript
export const Dashboard: React.FC = () => {
  const [selectedTab, setSelectedTab] = useState('overview');
  const [selectedTimeRange, setSelectedTimeRange] = useState('1h');
  
  // Real-time WebSocket connection
  const { connectionState, reconnect } = useRealtimeMetrics({ enabled: true });
  
  return (
    <div className="dashboard">
      <Header>
        â”œâ”€â–º ConnectionStatusIndicator (WebSocket status)
        â”œâ”€â–º ThemeToggle
        â””â”€â–º TimeRangeSelector
      </Header>
      
      <TabNavigation>
        â”œâ”€â–º Tab: Overview (default)
        â”œâ”€â–º Tab: Custom
        â”œâ”€â–º Tab: Services
        â”œâ”€â–º Tab: Dependencies
        â”œâ”€â–º Tab: Devices
        â”œâ”€â–º Tab: Events â† Shows real-time event stream
        â”œâ”€â–º Tab: Logs
        â”œâ”€â–º Tab: Sports
        â”œâ”€â–º Tab: Data Sources
        â”œâ”€â–º Tab: Analytics
        â”œâ”€â–º Tab: Alerts
        â””â”€â–º Tab: Configuration
      </TabNavigation>
      
      <TabContent>
        â””â”€â–º {TabComponent}
            â””â”€â–º Example: EventsTab
                â”œâ”€â–º const { events, loading } = useEvents(timeRange)
                â”œâ”€â–º useEffect(() => {
                â”‚     // Fetch events on mount and time range change
                â”‚     apiService.getRecentEvents({
                â”‚       limit: 100,
                â”‚       start_time: calculateStartTime(timeRange)
                â”‚     })
                â”‚   })
                â”‚
                â””â”€â–º return (
                      <EventTable>
                        {events.map(event => (
                          <EventRow key={event.event_id}>
                            â”œâ”€â–º Timestamp: {formatTimestamp(event.timestamp)}
                            â”œâ”€â–º Entity: {event.entity_id}
                            â”œâ”€â–º State: {event.old_state} â†’ {event.state}
                            â””â”€â–º Attributes: {JSON.stringify(event.attributes)}
                          </EventRow>
                        ))}
                      </EventTable>
                    )
      </TabContent>
    </div>
  );
};
```

---

#### 6.4 Real-time WebSocket Updates

**File**: `services/health-dashboard/src/hooks/useRealtimeMetrics.ts`

```typescript
export const useRealtimeMetrics = ({ enabled }: { enabled: boolean }) => {
  const [connectionState, setConnectionState] = useState<'connected' | 'disconnected'>('disconnected');
  const [realtimeData, setRealtimeData] = useState<any>(null);
  
  useEffect(() => {
    if (!enabled) return;
    
    // WebSocket connection to Admin API
    const ws = new WebSocket('ws://localhost:8003/api/ws/metrics');
    
    ws.onopen = () => {
      â””â”€â–º setConnectionState('connected')
          â””â”€â–º console.log('WebSocket connected')
    };
    
    ws.onmessage = (event) => {
      â””â”€â–º const data = JSON.parse(event.data)
          â””â”€â–º setRealtimeData(data)  // Triggers re-render
              â””â”€â–º Update UI components with new data
    };
    
    ws.onerror = (error) => {
      â””â”€â–º console.error('WebSocket error:', error)
          â””â”€â–º setConnectionState('disconnected')
    };
    
    ws.onclose = () => {
      â””â”€â–º setConnectionState('disconnected')
          â””â”€â–º Attempt reconnection after delay
    };
    
    return () => ws.close();  // Cleanup
  }, [enabled]);
  
  return { connectionState, realtimeData, reconnect: () => { /* ... */ } };
};
```

**WebSocket Message Structure**:
```json
{
  "type": "metrics_update",
  "timestamp": "2025-10-13T10:30:00.123Z",
  "data": {
    "event_count": 12500,
    "event_rate": 3.47,
    "queue_depth": 42,
    "processing_time_ms": 12.5
  }
}
```

---

## ğŸ“ˆ Performance Characteristics

### Event Processing Throughput

| Stage | Component | Throughput | Latency | Bottleneck |
|-------|-----------|------------|---------|------------|
| **Reception** | WebSocket Client | ~10,000 events/sec | <1ms | Network bandwidth |
| **Validation** | Event Processor | ~50,000 events/sec | <0.1ms | CPU-bound |
| **Queue** | Async Queue | ~100,000 events/sec | <0.01ms | Memory-bound |
| **Batch Processing** | Batch Processor | ~20,000 events/sec | ~5ms | Wait for batch |
| **Database Write** | InfluxDB Writer | ~10,000 points/sec | ~50ms | Disk I/O |
| **API Query** | Admin API | ~1,000 queries/sec | ~20ms | InfluxDB query |
| **Dashboard Render** | React Frontend | ~60 FPS | ~16ms | Browser render |

### Memory Usage

| Component | Base Memory | Peak Memory | Notes |
|-----------|------------|-------------|-------|
| WebSocket Ingestion | 50 MB | 200 MB | Event queue size |
| Enrichment Pipeline | 30 MB | 100 MB | Normalization buffers |
| Admin API | 40 MB | 150 MB | Query result caching |
| Health Dashboard | 80 MB | 300 MB | React state + charts |
| InfluxDB | 500 MB | 2 GB | Database cache |

---

## ğŸ” Key Data Structures

### Event Data Model (Throughout Pipeline)

```python
# Python (Backend Services)
{
    "event_type": "state_changed",
    "entity_id": "sensor.temperature",
    "domain": "sensor",
    "time_fired": "2025-10-13T10:30:00.123456+00:00",
    "origin": "LOCAL",
    "context": {
        "id": "01234567890abcdef",
        "user_id": None,
        "parent_id": None
    },
    "old_state": {
        "state": "20.5",
        "attributes": {
            "unit_of_measurement": "Â°C",
            "device_class": "temperature",
            "friendly_name": "Living Room Temperature"
        },
        "last_changed": "2025-10-13T10:25:00.000000+00:00",
        "last_updated": "2025-10-13T10:25:00.000000+00:00"
    },
    "new_state": {
        "state": "21.0",
        "attributes": {
            "unit_of_measurement": "Â°C",
            "device_class": "temperature",
            "friendly_name": "Living Room Temperature"
        },
        "last_changed": "2025-10-13T10:30:00.123456+00:00",
        "last_updated": "2025-10-13T10:30:00.123456+00:00"
    }
}
```

```typescript
// TypeScript (Frontend)
interface Event {
  event_id: string;
  event_type: 'state_changed' | 'call_service' | 'automation_triggered';
  entity_id: string;
  domain: string;
  timestamp: string; // ISO 8601
  state: string;
  old_state: string;
  attributes: Record<string, any>;
  context: {
    id: string;
    user_id: string | null;
  };
}
```

---

## ğŸš€ Optimization Points

### Current Optimizations

1. **Async Processing**: Non-blocking event handling with 10 concurrent workers
2. **Batch Writes**: Accumulate 1,000 events before writing to InfluxDB
3. **Connection Pooling**: Reuse InfluxDB connections across requests
4. **Query Caching**: Cache frequently accessed statistics in Admin API
5. **WebSocket Streaming**: Real-time updates without polling overhead
6. **React Memo**: Prevent unnecessary re-renders in dashboard components

### Potential Future Optimizations

1. **Redis Caching**: Cache hot data paths (last 1000 events, current stats)
2. **GraphQL**: Replace REST API with GraphQL for flexible queries
3. **Server-Sent Events (SSE)**: Alternative to WebSocket for one-way streaming
4. **Time-Series Downsampling**: Pre-aggregate older data for faster queries
5. **CDN for Static Assets**: Offload dashboard static files to CDN
6. **Database Sharding**: Partition InfluxDB by domain or time range

---

## ğŸ› ï¸ Troubleshooting Guide

### Common Issues & Debugging

#### Issue: Events not appearing in dashboard

**Debug Steps**:
1. Check WebSocket connection: `services/websocket-ingestion/logs`
   - Look for "Connected to Home Assistant" message
   - Verify subscription confirmation

2. Check event processing: Search logs for event_id
   ```bash
   grep "entity_id" services/websocket-ingestion/logs/app.log
   ```

3. Check InfluxDB write: Query InfluxDB directly
   ```flux
   from(bucket: "ha_events")
     |> range(start: -5m)
     |> filter(fn: (r) => r._measurement == "home_assistant_events")
     |> count()
   ```

4. Check Admin API: Test endpoint directly
   ```bash
   curl http://localhost:8003/api/events?limit=10
   ```

5. Check Dashboard: Browser console for API errors
   ```javascript
   // Check Network tab in DevTools
   // Look for failed /api/events requests
   ```

---

#### Issue: High latency (events delayed)

**Debug Steps**:
1. Check queue depth:
   ```python
   # In async_event_processor.py
   logger.info(f"Queue depth: {self.event_queue.qsize()}")
   ```

2. Check batch processing time:
   ```python
   # In batch_processor.py
   logger.info(f"Batch write time: {processing_time_ms}ms")
   ```

3. Check InfluxDB performance:
   ```bash
   # InfluxDB metrics
   curl http://localhost:8086/metrics
   ```

4. Monitor system resources:
   ```bash
   docker stats
   ```

---

#### Issue: Missing data in InfluxDB

**Debug Steps**:
1. Check InfluxDB write errors:
   ```python
   # In influxdb_batch_writer.py
   logger.error(f"Write failed: {e}")
   logger.info(f"Failed points: {self.total_points_failed}")
   ```

2. Verify InfluxDB bucket exists:
   ```bash
   docker exec influxdb influx bucket list
   ```

3. Check retention policy:
   ```bash
   docker exec influxdb influx bucket find --name ha_events
   ```

4. Verify write permissions:
   ```bash
   # Test write with InfluxDB CLI
   docker exec influxdb influx write ...
   ```

---

## ğŸ“Š Monitoring & Observability

### Key Metrics to Monitor

1. **Event Processing**
   - Events received per second
   - Events processed per second
   - Events failed per second
   - Queue depth (current/max)
   - Processing latency (p50, p95, p99)

2. **Database Performance**
   - Write throughput (points/sec)
   - Query latency (ms)
   - Failed writes count
   - Disk usage

3. **API Performance**
   - Request rate (requests/sec)
   - Response time (p50, p95, p99)
   - Error rate (%)
   - Active connections

4. **Frontend Performance**
   - Page load time
   - Time to interactive
   - WebSocket connection uptime
   - API call duration

### Logging Standards

All services use structured logging with correlation IDs:

```python
log_with_context(
    logger, "INFO", "Event processed successfully",
    operation="event_processing",
    correlation_id=correlation_id,
    event_type=event_type,
    entity_id=entity_id,
    processing_time_ms=processing_time
)
```

This enables distributed tracing across the entire event flow.

---

## ğŸ¯ Summary

This call tree demonstrates the complete journey of a Home Assistant event:

1. **WebSocket Reception** (~1ms): Event arrives from HA via WebSocket
2. **Validation & Extraction** (~0.1ms): Event data validated and structured
3. **Async Queue** (~0.01ms): Event added to processing queue
4. **Batch Accumulation** (~5s): Events accumulated into batches of 1,000
5. **Database Write** (~50ms): Batch written to InfluxDB time-series database
6. **API Query** (~20ms): Dashboard queries events via REST API
7. **Frontend Render** (~16ms): React components display events with 60 FPS

**Total End-to-End Latency**: ~5-6 seconds (dominated by batching strategy)
**Real-time Updates**: <100ms via WebSocket streaming

The system is designed for high throughput (10,000+ events/sec) with low resource usage through batching, async processing, and efficient data structures.

---

**Document Maintenance**: Update this document when:
- New services are added to the pipeline
- Event processing logic changes
- Database schema is modified
- API endpoints are added/changed
- Performance characteristics change significantly

