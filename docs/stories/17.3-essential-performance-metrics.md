# Story 17.3: Essential Performance Metrics

**Story ID**: 17.3  
**Epic**: Epic 17 - Essential Monitoring & Observability  
**Priority**: High  
**Status**: Ready for Development  
**Estimated Effort**: 1 week  
**Assignee**: TBD  

---

## ðŸŽ¯ Story Overview

Collect and display essential performance metrics for system monitoring, including response times, throughput, and basic resource usage to provide visibility into system performance and identify potential issues.

**Why This Story:**
- Essential for monitoring system performance and identifying bottlenecks
- Required for proactive performance management and optimization
- Foundation for performance-based alerting and capacity planning
- Provides visibility into system behavior and trends

---

## ðŸ“‹ Acceptance Criteria

### Primary Requirements
- [ ] Response time tracking for all API endpoints
- [ ] Throughput metrics (requests per minute, events per minute)
- [ ] Basic resource usage monitoring (CPU, memory, disk)
- [ ] Metrics are displayed in dashboard
- [ ] Performance trends are visible over time

### Technical Requirements
- [ ] Performance metrics collection integrated with all services
- [ ] Metrics stored in InfluxDB with appropriate retention
- [ ] Performance dashboard integrated with health dashboard
- [ ] Metrics collection overhead <2% of service performance
- [ ] Performance trends calculated and displayed

### Quality Requirements
- [ ] Performance metrics are accurate and reliable
- [ ] Metrics collection doesn't impact service performance
- [ ] Performance trends are meaningful and actionable
- [ ] Performance monitoring system is fault-tolerant

---

## ðŸ—ï¸ Technical Implementation

### Architecture
```
Services â†’ Performance Metrics Collector â†’ InfluxDB â†’ Performance Dashboard
```

### Components
1. **Performance Metrics Collector**: Collects performance data from all services
2. **Metrics Storage**: InfluxDB schema for performance metrics storage
3. **Performance Calculator**: Calculates trends and aggregates
4. **Performance Dashboard**: Enhanced dashboard with performance views

### Technology Choices
- **Metrics Collection**: Custom metrics collection (building on existing patterns)
- **Storage**: InfluxDB for performance metrics (existing infrastructure)
- **Calculation**: Python-based metrics calculation and aggregation
- **Display**: Enhanced health dashboard with performance views

---

## ðŸ“Š Detailed Requirements

### 1. Performance Metrics Collection
**Metrics Collected:**
- **API Response Times**: Per endpoint, per service
- **Throughput**: Requests per minute, events per minute
- **Resource Usage**: CPU, memory, disk usage per service
- **Queue Metrics**: Processing queue sizes, wait times
- **Error Rates**: Request failure rates, error types

**Metrics Structure:**
```python
@dataclass
class PerformanceMetrics:
    timestamp: datetime
    service: str
    endpoint: Optional[str]
    response_time_ms: float
    requests_per_minute: int
    events_per_minute: int
    cpu_usage_percent: float
    memory_usage_mb: float
    disk_usage_percent: float
    error_rate_percent: float
    queue_size: int
    active_connections: int
```

### 2. API Response Time Tracking
**Response Time Collection:**
```python
class ResponseTimeTracker:
    def track_request(self, endpoint: str, duration_ms: float):
        """Track API request response time"""
        metrics = PerformanceMetrics(
            timestamp=datetime.utcnow(),
            service=self.service_name,
            endpoint=endpoint,
            response_time_ms=duration_ms,
            requests_per_minute=self.calculate_rpm(),
            events_per_minute=self.calculate_epm(),
            cpu_usage_percent=psutil.cpu_percent(),
            memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
            disk_usage_percent=psutil.disk_usage('/').percent,
            error_rate_percent=self.calculate_error_rate(),
            queue_size=self.get_queue_size(),
            active_connections=self.get_active_connections()
        )
        
        self.store_metrics(metrics)
```

**Response Time Tracking Integration:**
- **FastAPI Services**: Middleware for automatic response time tracking
- **aiohttp Services**: Middleware for WebSocket and HTTP response tracking
- **React Dashboard**: Client-side performance tracking for API calls

### 3. Throughput Metrics Collection
**Throughput Calculation:**
```python
class ThroughputCalculator:
    def __init__(self):
        self.request_times = deque(maxlen=60)  # Last 60 seconds
        self.event_times = deque(maxlen=60)
    
    def record_request(self):
        """Record a request timestamp"""
        self.request_times.append(time.time())
    
    def record_event(self):
        """Record an event processing timestamp"""
        self.event_times.append(time.time())
    
    def get_requests_per_minute(self) -> int:
        """Calculate requests per minute"""
        now = time.time()
        # Count requests in last 60 seconds
        return sum(1 for t in self.request_times if now - t <= 60)
    
    def get_events_per_minute(self) -> int:
        """Calculate events per minute"""
        now = time.time()
        # Count events in last 60 seconds
        return sum(1 for t in self.event_times if now - t <= 60)
```

### 4. Resource Usage Monitoring
**Resource Monitoring:**
```python
class ResourceMonitor:
    def get_cpu_usage(self) -> float:
        """Get current CPU usage percentage"""
        return psutil.cpu_percent(interval=1)
    
    def get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        memory = psutil.virtual_memory()
        return memory.used / 1024 / 1024
    
    def get_disk_usage(self) -> float:
        """Get current disk usage percentage"""
        disk = psutil.disk_usage('/')
        return (disk.used / disk.total) * 100
    
    def get_service_specific_metrics(self, service_name: str) -> Dict[str, float]:
        """Get service-specific resource metrics"""
        try:
            process = psutil.Process()
            return {
                "cpu_percent": process.cpu_percent(),
                "memory_mb": process.memory_info().rss / 1024 / 1024,
                "num_threads": process.num_threads(),
                "num_fds": process.num_fds() if hasattr(process, 'num_fds') else 0
            }
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            return {}
```

### 5. InfluxDB Schema for Performance Metrics
**Measurement Structure:**
```python
# InfluxDB measurement: performance_metrics
{
    "measurement": "performance_metrics",
    "tags": {
        "service": "admin-api",
        "endpoint": "/api/health",
        "method": "GET"
    },
    "fields": {
        "response_time_ms": 25.5,
        "requests_per_minute": 45,
        "events_per_minute": 120,
        "cpu_usage_percent": 15.2,
        "memory_usage_mb": 256.8,
        "disk_usage_percent": 45.3,
        "error_rate_percent": 0.5,
        "queue_size": 0,
        "active_connections": 3
    },
    "timestamp": "2025-10-12T22:58:40Z"
}
```

**Retention Policy:**
- **Raw Metrics**: 7 days retention
- **Aggregated Metrics**: 30 days retention (hourly aggregates)
- **Performance Trends**: 90 days retention (daily aggregates)

---

## ðŸ”§ Implementation Steps

### Step 1: Metrics Collection Foundation (Day 1-2)
1. Create `PerformanceMetrics` data structure
2. Implement basic metrics collection logic
3. Add response time tracking middleware
4. Test metrics collection with sample requests

### Step 2: Throughput and Resource Monitoring (Day 3-4)
1. Implement throughput calculation logic
2. Add resource usage monitoring
3. Integrate metrics collection with all services
4. Test throughput and resource metrics accuracy

### Step 3: InfluxDB Storage and Aggregation (Day 5)
1. Design InfluxDB schema for performance metrics
2. Implement metrics storage logic
3. Add performance trend calculation
4. Test metrics storage and retrieval

### Step 4: Dashboard Integration (Day 6-7)
1. Add performance views to health dashboard
2. Implement performance trend visualization
3. Add performance alerting integration
4. Test dashboard performance display

---

## ðŸ“ˆ Success Metrics

### Technical Metrics
- **Metrics Coverage**: 100% of services and endpoints tracked
- **Collection Overhead**: <2% performance impact from metrics collection
- **Storage Efficiency**: Metrics stored with appropriate retention
- **Trend Accuracy**: Performance trends calculated accurately

### Operational Metrics
- **Performance Visibility**: Clear performance trends and patterns visible
- **Issue Detection**: Performance issues detected within 5 minutes
- **Trend Analysis**: Performance trends tracked over meaningful time periods
- **Actionable Insights**: Metrics provide actionable performance information

---

## ðŸš« Non-Goals (Avoiding Over-Engineering)

### What We're NOT Doing
- Complex performance profiling and analysis
- Advanced performance prediction and forecasting
- Complex performance dashboards beyond basic metrics display
- Advanced performance analytics and reporting
- Integration with external performance monitoring platforms
- Complex performance optimization recommendations

### Why These Are Out of Scope
- **Simplicity**: Focus on essential performance metrics only
- **Performance**: Avoid complex metrics collection that impacts throughput
- **Maintenance**: Keep performance monitoring simple and maintainable
- **Time**: Deliver value quickly without over-engineering
- **Personal Project**: Appropriate scope for home automation system

---

## ðŸ” Risk Assessment

### Low Risk
- **Technical Implementation**: Building on existing monitoring infrastructure
- **Integration**: Services already have basic monitoring capabilities
- **Performance**: Simple metrics collection has minimal overhead

### Medium Risk
- **Performance Impact**: Metrics collection shouldn't impact service performance
- **Storage Growth**: Performance metrics storage needs proper retention management

### Mitigation Strategies
- Implement efficient metrics collection with minimal overhead
- Use appropriate retention policies for metrics storage
- Monitor metrics collection performance impact
- Optimize metrics collection frequency and storage

---

## ðŸ“š Dependencies

### Prerequisites
- InfluxDB infrastructure (Epic 3)
- Health dashboard (Epic 5)
- Enhanced health monitoring (Story 17.2)

### Blockers
- None identified

### Related Work
- Enhanced health monitoring (Story 17.2) - for performance health indicators
- Critical alerting system (Story 17.4) - for performance-based alerting
- Health dashboard (Epic 5) - for performance display integration

---

## âœ… Definition of Done

### Technical Requirements
- [ ] Performance metrics collection implemented for all services
- [ ] Response time tracking for all API endpoints
- [ ] Throughput metrics collection and calculation
- [ ] Resource usage monitoring implemented
- [ ] InfluxDB schema designed and implemented for performance metrics
- [ ] Performance dashboard integrated with health dashboard
- [ ] Metrics collection overhead <2% of service performance
- [ ] All performance monitoring functionality tested and documented

### Quality Requirements
- [ ] Performance metrics are accurate and reliable
- [ ] Metrics collection doesn't impact service performance
- [ ] Performance trends are meaningful and actionable
- [ ] Performance monitoring system is fault-tolerant
- [ ] Metrics storage is efficient and properly retained

### Business Requirements
- [ ] System performance is clearly visible to users
- [ ] Performance issues are detected and reported promptly
- [ ] Performance trends support capacity planning and optimization
- [ ] Performance monitoring improves system reliability

---

## ðŸ“ Testing Strategy

### Unit Testing
- Test performance metrics data structures and calculations
- Test throughput calculation logic
- Test resource monitoring functions

### Integration Testing
- Test metrics collection integration with all services
- Test metrics storage and retrieval from InfluxDB
- Test performance dashboard display and updates

### End-to-End Testing
- Test complete performance monitoring workflow
- Test performance metrics accuracy under various loads
- Test performance monitoring performance impact

---

## ðŸ“‹ Performance Metrics Summary

### Metrics Collected
- **Response Times**: API endpoint response times per service
- **Throughput**: Requests per minute, events per minute
- **Resource Usage**: CPU, memory, disk usage per service
- **Queue Metrics**: Processing queue sizes, wait times
- **Error Rates**: Request failure rates, error types
- **Connection Metrics**: Active connections, connection pools

### Performance Trends
- **Hourly Aggregates**: Average response times, peak throughput
- **Daily Aggregates**: Performance trends, resource usage patterns
- **Weekly Aggregates**: Long-term performance trends and capacity planning

### Performance Thresholds
- **Response Time**: <500ms for API endpoints
- **Throughput**: >10 requests/minute for active services
- **Resource Usage**: <80% CPU, <80% memory, <90% disk
- **Error Rate**: <5% error rate for all endpoints

---

**Story Owner**: BMAD Master  
**Created**: October 12, 2025  
**Last Updated**: October 12, 2025  
**Status**: Ready for Development
